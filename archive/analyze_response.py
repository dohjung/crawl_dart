#!/usr/bin/env python3
"""
DART ì‘ë‹µ ìƒì„¸ ë¶„ì„
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

def analyze_search_response():
    """ê²€ìƒ‰ ì‘ë‹µ ìƒì„¸ ë¶„ì„"""
    print("ğŸ” DART ê²€ìƒ‰ ì‘ë‹µ ìƒì„¸ ë¶„ì„")
    print("=" * 50)
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    })
    
    # ì„¸ì…˜ ì´ˆê¸°í™”
    main_url = "https://dart.fss.or.kr/dsab007/main.do"
    session.get(main_url)
    
    # ê²€ìƒ‰ ìš”ì²­
    search_url = "https://dart.fss.or.kr/dsab007/detailSearch.ax"
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=90)  # ìµœê·¼ 3ê°œì›”
    
    params = {
        'option': 'corp',
        'textCrpNm': 'ì‚¼ì„±ì „ì',
        'startDate': start_date.strftime('%Y%m%d'),
        'endDate': end_date.strftime('%Y%m%d'),
        'currentPage': '1',
        'pageCount': '20'
    }
    
    headers = {
        'Content-Type': 'application/x-www-form-urlencoded',
        'X-Requested-With': 'XMLHttpRequest',
        'Referer': main_url
    }
    
    try:
        response = session.post(search_url, data=params, headers=headers)
        print(f"âœ“ ì‘ë‹µ ìˆ˜ì‹ : {response.status_code}, ê¸¸ì´: {len(response.text)}")
        
        # BeautifulSoupë¡œ íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # í…Œì´ë¸” ì°¾ê¸°
        tables = soup.find_all('table')
        print(f"\nğŸ“Š í…Œì´ë¸” ê°œìˆ˜: {len(tables)}")
        
        for i, table in enumerate(tables):
            print(f"\ní…Œì´ë¸” {i+1}:")
            print(f"  í´ë˜ìŠ¤: {table.get('class', [])}")
            
            # í—¤ë” í–‰ ë¶„ì„
            thead = table.find('thead')
            if thead:
                header_cells = thead.find_all(['th', 'td'])
                print(f"  í—¤ë” ê°œìˆ˜: {len(header_cells)}")
                for j, cell in enumerate(header_cells):
                    text = cell.get_text(strip=True)
                    print(f"    {j+1}: '{text}'")
            
            # ë³¸ë¬¸ í–‰ ë¶„ì„
            tbody = table.find('tbody')
            if tbody:
                rows = tbody.find_all('tr')
                print(f"  ë°ì´í„° í–‰ ê°œìˆ˜: {len(rows)}")
                
                # ì²« ë²ˆì§¸ í–‰ ìƒì„¸ ë¶„ì„
                if rows:
                    first_row = rows[0]
                    cells = first_row.find_all(['td', 'th'])
                    print(f"  ì²« ë²ˆì§¸ í–‰ ì…€ ê°œìˆ˜: {len(cells)}")
                    
                    for j, cell in enumerate(cells):
                        text = cell.get_text(strip=True)[:50]
                        
                        # ë§í¬ ì°¾ê¸°
                        links = cell.find_all('a')
                        if links:
                            for link in links:
                                href = link.get('href', '')
                                onclick = link.get('onclick', '')
                                print(f"    ì…€ {j+1}: '{text}' â†’ ë§í¬: href='{href}', onclick='{onclick[:50]}...'")
                        else:
                            print(f"    ì…€ {j+1}: '{text}'")
            else:
                # tbodyê°€ ì—†ëŠ” ê²½ìš° tr ì§ì ‘ ì°¾ê¸°
                rows = table.find_all('tr')
                data_rows = rows[1:] if len(rows) > 1 else []
                print(f"  ë°ì´í„° í–‰ ê°œìˆ˜: {len(data_rows)}")
                
                if data_rows:
                    first_row = data_rows[0]
                    cells = first_row.find_all(['td', 'th'])
                    print(f"  ì²« ë²ˆì§¸ ë°ì´í„° í–‰ ì…€ ê°œìˆ˜: {len(cells)}")
                    
                    for j, cell in enumerate(cells):
                        text = cell.get_text(strip=True)[:30]
                        
                        # ë§í¬ ì°¾ê¸°
                        links = cell.find_all('a')
                        if links:
                            for link in links:
                                href = link.get('href', '')
                                onclick = link.get('onclick', '')
                                print(f"    ì…€ {j+1}: '{text}' â†’ href='{href}', onclick='{onclick[:80]}...'")
                        else:
                            print(f"    ì…€ {j+1}: '{text}'")
        
        # ì „ì²´ ë§í¬ ë¶„ì„
        print(f"\nğŸ”— ì „ì²´ ë§í¬ ë¶„ì„:")
        all_links = soup.find_all('a')
        print(f"  ì´ ë§í¬ ê°œìˆ˜: {len(all_links)}")
        
        report_links = []
        for link in all_links:
            onclick = link.get('onclick', '')
            if 'openReportViewer' in onclick:
                report_links.append({
                    'text': link.get_text(strip=True)[:40],
                    'onclick': onclick,
                    'href': link.get('href', '')
                })
        
        print(f"  ë³´ê³ ì„œ ë·°ì–´ ë§í¬: {len(report_links)}ê°œ")
        for i, link in enumerate(report_links[:3]):  # ì²˜ìŒ 3ê°œë§Œ
            print(f"    {i+1}: '{link['text']}' â†’ {link['onclick'][:60]}...")
    
    except Exception as e:
        print(f"âœ— ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    analyze_search_response()