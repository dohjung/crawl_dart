#!/usr/bin/env python3
"""
A01,A02,A03 ì‘ë‹µ ìƒì„¸ í™•ì¸
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

def check_regular_response():
    """ì •ê¸°ê³µì‹œ ì‘ë‹µ ìƒì„¸ í™•ì¸"""
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    })
    
    # ì„¸ì…˜ ì´ˆê¸°í™”
    main_url = "https://dart.fss.or.kr/dsab007/main.do"
    session.get(main_url)
    
    search_url = "https://dart.fss.or.kr/dsab007/detailSearch.ax"
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=10*365)
    
    headers = {
        'Content-Type': 'application/x-www-form-urlencoded',
        'X-Requested-With': 'XMLHttpRequest',
        'Referer': main_url
    }
    
    params = {
        'option': 'corp',
        'textCrpNm': 'ì‚¼ì„±ì „ì',
        'startDate': start_date.strftime('%Y%m%d'),
        'endDate': end_date.strftime('%Y%m%d'),
        'publicType': 'A01,A02,A03',
        'currentPage': '1',
        'pageCount': '100'
    }
    
    print("ğŸ” ì •ê¸°ê³µì‹œ ì‘ë‹µ ìƒì„¸ ë¶„ì„")
    print("=" * 50)
    print(f"íŒŒë¼ë¯¸í„°: {params}")
    print()
    
    try:
        response = session.post(search_url, data=params, headers=headers)
        print(f"ì‘ë‹µ ì½”ë“œ: {response.status_code}")
        print(f"ì‘ë‹µ ê¸¸ì´: {len(response.text)}")
        
        # ì „ì²´ ì‘ë‹µ í™•ì¸
        print(f"\nğŸ“„ ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 500ì):")
        print("-" * 30)
        print(response.text[:500])
        
        # BeautifulSoupë¡œ íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ëª¨ë“  í…Œì´ë¸” ì°¾ê¸°
        tables = soup.find_all('table')
        print(f"\nğŸ“Š í…Œì´ë¸” ê°œìˆ˜: {len(tables)}")
        
        for i, table in enumerate(tables):
            table_class = table.get('class', [])
            print(f"  í…Œì´ë¸” {i+1}: class={table_class}")
            
            # tbody ì°¾ê¸°
            tbody = table.find('tbody')
            if tbody:
                rows = tbody.find_all('tr')
                print(f"    tbody í–‰ ìˆ˜: {len(rows)}")
                
                if rows:
                    for j, row in enumerate(rows[:2]):  # ì²˜ìŒ 2ê°œ í–‰ë§Œ
                        cells = row.find_all('td')
                        print(f"    í–‰ {j+1}: {len(cells)}ê°œ ì…€")
                        for k, cell in enumerate(cells):
                            text = cell.get_text(strip=True)[:30]
                            link = cell.find('a')
                            if link:
                                href = link.get('href', '')
                                onclick = link.get('onclick', '')[:50]
                                print(f"      ì…€ {k+1}: '{text}' [ë§í¬: {href}, onclick: {onclick}...]")
                            else:
                                print(f"      ì…€ {k+1}: '{text}'")
            else:
                # tbody ì—†ì´ tr ì§ì ‘ ì°¾ê¸°
                rows = table.find_all('tr')
                data_rows = rows[1:] if len(rows) > 1 else []
                print(f"    ì§ì ‘ tr í–‰ ìˆ˜: {len(data_rows)}")
                
                if data_rows:
                    first_row = data_rows[0]
                    cells = first_row.find_all('td')
                    print(f"    ì²« í–‰ ì…€ ìˆ˜: {len(cells)}")
                    for k, cell in enumerate(cells):
                        text = cell.get_text(strip=True)[:30]
                        link = cell.find('a')
                        if link:
                            href = link.get('href', '')
                            onclick = link.get('onclick', '')[:50]
                            print(f"      ì…€ {k+1}: '{text}' [ë§í¬: {href}, onclick: {onclick}...]")
                        else:
                            print(f"      ì…€ {k+1}: '{text}'")
        
        # ëª¨ë“  ë§í¬ í™•ì¸
        all_links = soup.find_all('a')
        report_links = []
        for link in all_links:
            onclick = link.get('onclick', '')
            if 'openReportViewer' in onclick:
                report_links.append({
                    'text': link.get_text(strip=True),
                    'onclick': onclick,
                    'href': link.get('href', '')
                })
        
        print(f"\nğŸ”— ë³´ê³ ì„œ ë·°ì–´ ë§í¬: {len(report_links)}ê°œ")
        for link in report_links:
            print(f"  - {link['text']}: {link['href']}")
            print(f"    onclick: {link['onclick']}")
            
    except Exception as e:
        print(f"ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    check_regular_response()