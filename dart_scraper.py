#!/usr/bin/env python3
"""
DART ê³µì‹œì •ë³´ ê²€ìƒ‰ í”„ë¡œê·¸ë¨
https://dart.fss.or.kr/dsab007/main.do ì‚¬ì´íŠ¸ì˜ íšŒì‚¬ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì½˜ì†”ì—ì„œ ì‚¬ìš©
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import json
import re
import os
from typing import List, Dict, Optional


class DartScraper:
    def __init__(self):
        self.session = requests.Session()
        self.base_url = "https://dart.fss.or.kr"
        self.main_url = f"{self.base_url}/dsab007/main.do"
        self.search_url = f"{self.base_url}/dsab007/detailSearch.ax"
        
        # ê¸°ë³¸ í—¤ë” ì„¤ì •
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def get_search_page(self) -> bool:
        """ê²€ìƒ‰ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ì„¸ì…˜ ì´ˆê¸°í™”"""
        try:
            response = self.session.get(self.main_url)
            response.raise_for_status()
            
            # HTML íŒŒì‹±í•˜ì—¬ í•„ìš”í•œ ì •ë³´ í™•ì¸
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²€ìƒ‰ í¼ì´ ìˆëŠ”ì§€ í™•ì¸
            search_form = soup.find('form')
            if search_form:
                print("âœ“ DART ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì† ì„±ê³µ")
                return True
            else:
                print("âœ— ê²€ìƒ‰ í¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
                
        except Exception as e:
            print(f"âœ— í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {e}")
            return False
    
    def search_company_regular_reports(self, company_name: str, max_pages: int = 20) -> List[Dict]:
        """
        íšŒì‚¬ëª…ìœ¼ë¡œ ì •ê¸°ê³µì‹œ ê²€ìƒ‰ (10ë…„, ì •ê¸°ê³µì‹œ ì²´í¬ë°•ìŠ¤ ì‚¬ìš©)
        
        Args:
            company_name: ê²€ìƒ‰í•  íšŒì‚¬ëª…
            max_pages: ìµœëŒ€ ê²€ìƒ‰í•  í˜ì´ì§€ ìˆ˜
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # 10ë…„ ê¸°ê°„ ì„¤ì •
            end_date = datetime.now()
            start_date = end_date - timedelta(days=10*365)
            
            print(f"ğŸ” '{company_name}' ì •ê¸°ê³µì‹œ ê²€ìƒ‰ ì¤‘... (ê¸°ê°„: 10ë…„)")
            print(f"   ê²€ìƒ‰ ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
            print(f"   ê³µì‹œìœ í˜•: ì •ê¸°ê³µì‹œ (ì‚¬ì—…ë³´ê³ ì„œ, ë°˜ê¸°ë³´ê³ ì„œ, ë¶„ê¸°ë³´ê³ ì„œ)")
            
            # Ajax ìš”ì²­ í—¤ë” ì„¤ì •
            ajax_headers = {
                'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
                'X-Requested-With': 'XMLHttpRequest',
                'Referer': self.main_url
            }
            
            all_results = []
            
            # ì—¬ëŸ¬ í˜ì´ì§€ ê²€ìƒ‰
            for page in range(1, max_pages + 1):
                # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì„¤ì • (íŠœí”Œ ë°°ì—´ ë°©ì‹ - ì„±ê³µ í™•ì¸ë¨)
                search_data = [
                    ('option', 'corp'),
                    ('textCrpNm', company_name),
                    ('startDate', start_date.strftime('%Y%m%d')),
                    ('endDate', end_date.strftime('%Y%m%d')),
                    ('publicType', 'A001'),  # ì‚¬ì—…ë³´ê³ ì„œ
                    ('publicType', 'A002'),  # ë°˜ê¸°ë³´ê³ ì„œ
                    ('publicType', 'A003'),  # ë¶„ê¸°ë³´ê³ ì„œ
                    ('currentPage', str(page)),
                    ('pageCount', '100')
                ]
                
                print(f"   í˜ì´ì§€ {page} ê²€ìƒ‰ ì¤‘...")
                
                # POST ìš”ì²­ìœ¼ë¡œ ê²€ìƒ‰ ì‹¤í–‰ (data íŒŒë¼ë¯¸í„° ì‚¬ìš©)
                response = self.session.post(self.search_url, data=search_data, headers=ajax_headers)
                response.raise_for_status()
                
                # ê²°ê³¼ íŒŒì‹±
                page_results = self._parse_search_results(response.text)
                
                if not page_results:
                    print(f"   í˜ì´ì§€ {page}: ê²°ê³¼ ì—†ìŒ - ê²€ìƒ‰ ì¢…ë£Œ")
                    break
                
                print(f"   í˜ì´ì§€ {page}: {len(page_results)}ê±´ ë°œê²¬")
                all_results.extend(page_results)
                
                # ì ì‹œ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(0.5)
            
            results = all_results
            print(f"   ì´ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê±´")
            
            return results
            
        except Exception as e:
            print(f"âœ— ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_company_all(self, company_name: str, years: int = 10, max_pages: int = 20) -> List[Dict]:
        """
        íšŒì‚¬ëª…ìœ¼ë¡œ ì „ì²´ ê³µì‹œì •ë³´ ê²€ìƒ‰ (í•„í„°ë§ ì—†ìŒ)
        
        Args:
            company_name: ê²€ìƒ‰í•  íšŒì‚¬ëª…
            years: ê²€ìƒ‰ ê¸°ê°„ (ë…„)
            max_pages: ìµœëŒ€ ê²€ìƒ‰í•  í˜ì´ì§€ ìˆ˜
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë‚ ì§œ ê³„ì‚°
            end_date = datetime.now()
            start_date = end_date - timedelta(days=years*365)
            
            print(f"ğŸ” '{company_name}' ì „ì²´ ê²€ìƒ‰ ì¤‘... (ê¸°ê°„: {years}ë…„)")
            print(f"   ê²€ìƒ‰ ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
            
            # Ajax ìš”ì²­ í—¤ë” ì„¤ì •
            ajax_headers = {
                'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
                'X-Requested-With': 'XMLHttpRequest',
                'Referer': self.main_url
            }
            
            all_results = []
            
            # ì—¬ëŸ¬ í˜ì´ì§€ ê²€ìƒ‰
            for page in range(1, max_pages + 1):
                # ê¸°ë³¸ ê²€ìƒ‰ íŒŒë¼ë¯¸í„° (ê°€ì¥ ë§ì€ ê²°ê³¼ ë°˜í™˜)
                search_params = {
                    'option': 'corp',
                    'textCrpNm': company_name,
                    'startDate': start_date.strftime('%Y%m%d'),
                    'endDate': end_date.strftime('%Y%m%d'),
                    'currentPage': str(page),
                    'pageCount': '100'
                }
                
                print(f"   í˜ì´ì§€ {page} ê²€ìƒ‰ ì¤‘...")
                
                # POST ìš”ì²­ìœ¼ë¡œ ê²€ìƒ‰ ì‹¤í–‰
                response = self.session.post(self.search_url, data=search_params, headers=ajax_headers)
                response.raise_for_status()
                
                # ê²°ê³¼ íŒŒì‹±
                page_results = self._parse_search_results(response.text)
                
                if not page_results:
                    print(f"   í˜ì´ì§€ {page}: ê²°ê³¼ ì—†ìŒ - ê²€ìƒ‰ ì¢…ë£Œ")
                    break
                
                print(f"   í˜ì´ì§€ {page}: {len(page_results)}ê±´ ë°œê²¬")
                all_results.extend(page_results)
                
                # ì ì‹œ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(0.5)
            
            print(f"   ì´ ê²€ìƒ‰ ê²°ê³¼: {len(all_results)}ê±´")
            return all_results
            
        except Exception as e:
            print(f"âœ— ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_search_results(self, html_content: str) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ HTML íŒŒì‹±"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            results = []
            
            # ê²°ê³¼ í…Œì´ë¸” ì°¾ê¸° (ì‹¤ì œ DART êµ¬ì¡°)
            table = soup.find('table', {'class': 'tbList'})
            if not table:
                # ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì°¾ê¸°
                table = soup.find('table')
                
            if table:
                tbody = table.find('tbody')
                if tbody:
                    rows = tbody.find_all('tr')
                    
                    for row in rows:
                        cols = row.find_all('td')
                        if len(cols) >= 6:  # ìµœì†Œ 6ê°œ ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•¨
                            try:
                                # ë³´ê³ ì„œëª… ì»¬ëŸ¼ì—ì„œ ë§í¬ ì¶”ì¶œ
                                report_cell = cols[2]
                                report_link = report_cell.find('a')
                                report_url = ''
                                
                                if report_link and report_link.get('href'):
                                    href = report_link.get('href')
                                    if href.startswith('/'):
                                        report_url = self.base_url + href
                                    elif href.startswith('http'):
                                        report_url = href
                                    else:
                                        report_url = self.base_url + '/' + href
                                
                                # ê° ì»¬ëŸ¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                result = {
                                    'no': cols[0].get_text(strip=True),
                                    'company': cols[1].get_text(strip=True),
                                    'report_name': cols[2].get_text(strip=True),
                                    'submitter': cols[3].get_text(strip=True),
                                    'submit_date': cols[4].get_text(strip=True),
                                    'note': cols[5].get_text(strip=True) if len(cols) > 5 else '',
                                    'report_url': report_url  # ë³´ê³ ì„œ ë§í¬ ì¶”ê°€
                                }
                                
                                # ë¹ˆ ê²°ê³¼ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                                if result['company'] and result['report_name']:
                                    results.append(result)
                                    
                            except Exception as e:
                                print(f"í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
                                continue
            
            # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° "ì¡°íšŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" ë©”ì‹œì§€ í™•ì¸
            if not results:
                no_result = soup.find(string=lambda text: text and 'ì¡°íšŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤' in text)
                if no_result:
                    print("ğŸ“­ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    print("âš ï¸ ê²°ê³¼ íŒŒì‹± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                    # ë””ë²„ê·¸ìš©: HTML ì¼ë¶€ ì¶œë ¥
                    print("HTML ìƒ˜í”Œ:")
                    print(soup.prettify()[:500] + "...")
            
            return results
            
        except Exception as e:
            print(f"âœ— ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def display_results(self, results: List[Dict]) -> None:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        if not results:
            print("ğŸ“­ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“Š ê²€ìƒ‰ ê²°ê³¼: ì´ {len(results)}ê±´")
        print("=" * 100)
        
        for i, result in enumerate(results, 1):
            print(f"{i:3d}. íšŒì‚¬: {result['company']}")
            print(f"     ë³´ê³ ì„œ: {result['report_name']}")
            print(f"     ì œì¶œì¸: {result['submitter']}")
            print(f"     ì ‘ìˆ˜ì¼: {result['submit_date']}")
            if result.get('report_url'):
                print(f"     ë§í¬: {result['report_url']}")
            if result['note']:
                print(f"     ë¹„ê³ : {result['note']}")
            print("-" * 80)
    
    def save_results(self, results: List[Dict], filename: str = None) -> None:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not results:
            print("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"dart_search_results_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âœ— íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def save_links_to_txt(self, results: List[Dict], filename: str = None) -> None:
        """í•„í„°ë§ëœ ë³´ê³ ì„œì˜ ë§í¬ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥"""
        if not results:
            print("ì €ì¥í•  ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë§í¬ê°€ ìˆëŠ” ê²°ê³¼ë§Œ í•„í„°ë§
        results_with_links = [r for r in results if r.get('report_url')]
        
        if not results_with_links:
            print("ë§í¬ê°€ ìˆëŠ” ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"dart_report_links_{timestamp}.txt"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"# DART ë³´ê³ ì„œ ë§í¬ ëª©ë¡\n")
                f.write(f"# ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# ì´ {len(results_with_links)}ê°œ ë§í¬\n")
                f.write(f"{'='*80}\n\n")
                
                for i, result in enumerate(results_with_links, 1):
                    f.write(f"[{i:3d}] {result['company']} - {result['report_name']}\n")
                    f.write(f"      ì ‘ìˆ˜ì¼: {result['submit_date']}\n")
                    f.write(f"      ë§í¬: {result['report_url']}\n")
                    if result.get('note'):
                        f.write(f"      ë¹„ê³ : {result['note']}\n")
                    f.write(f"\n")
                
                # ë§í¬ë§Œ ë³„ë„ë¡œ ì •ë¦¬
                f.write(f"\n{'='*80}\n")
                f.write(f"# ë§í¬ë§Œ ë³„ë„ ì •ë¦¬ (ë³µì‚¬ìš©)\n")
                f.write(f"{'='*80}\n\n")
                
                for result in results_with_links:
                    f.write(f"{result['report_url']}\n")
            
            print(f"ğŸ”— ë§í¬ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {len(results_with_links)}ê°œ)")
            
        except Exception as e:
            print(f"âœ— ë§í¬ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_report_download_info(self, report_url: str) -> Optional[Dict[str, str]]:
        """ë³´ê³ ì„œ í˜ì´ì§€ì—ì„œ ë‹¤ìš´ë¡œë“œ ì •ë³´ ì¶”ì¶œ"""
        try:
            print(f"ğŸ“„ ë³´ê³ ì„œ í˜ì´ì§€ ë¶„ì„: {report_url}")
            
            response = self.session.get(report_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # rcpNo ì¶”ì¶œ (URLì—ì„œ)
            rcp_no_match = re.search(r'rcpNo=(\d+)', report_url)
            rcp_no = rcp_no_match.group(1) if rcp_no_match else None
            
            if not rcp_no:
                print("  âŒ rcpNoë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
            
            # dcmNo ì°¾ê¸° - ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„
            dcm_no = None
            
            # ë°©ë²• 1: JavaScript ë³€ìˆ˜ì—ì„œ ì°¾ê¸°
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string:
                    content = script.string
                    
                    # dcmNo ë³€ìˆ˜ ì°¾ê¸°
                    dcm_match = re.search(r'dcmNo\s*[=:]\s*["\']?(\d+)["\']?', content, re.IGNORECASE)
                    if dcm_match:
                        dcm_no = dcm_match.group(1)
                        print(f"  âœ… JavaScript ë³€ìˆ˜ì—ì„œ dcmNo ë°œê²¬: {dcm_no}")
                        break
                    
                    # openPdfDownload í•¨ìˆ˜ í˜¸ì¶œ ì°¾ê¸°
                    pdf_match = re.search(r'openPdfDownload\s*\(\s*["\']?(\d+)["\']?\s*,\s*["\']?(\d+)["\']?\s*\)', content)
                    if pdf_match:
                        rcp_param = pdf_match.group(1)
                        dcm_param = pdf_match.group(2)
                        print(f"  âœ… openPdfDownload í˜¸ì¶œì—ì„œ ë°œê²¬: rcpNo={rcp_param}, dcmNo={dcm_param}")
                        dcm_no = dcm_param  # openPdfDownloadì—ì„œ ì°¾ì€ dcmNoê°€ ì •í™•í•¨
                        break
            
            # ë°©ë²• 2: HTML ì†ì„±ì—ì„œ ì°¾ê¸°
            if not dcm_no:
                elements_with_dcm = soup.find_all(attrs={'data-dcm-no': True})
                if elements_with_dcm:
                    dcm_no = elements_with_dcm[0]['data-dcm-no']
                    print(f"  âœ… HTML data-dcm-no ì†ì„±ì—ì„œ ë°œê²¬: {dcm_no}")
            
            # ë°©ë²• 3: ìˆ¨ê²¨ì§„ input í•„ë“œì—ì„œ ì°¾ê¸°
            if not dcm_no:
                hidden_inputs = soup.find_all('input', {'type': 'hidden'})
                for inp in hidden_inputs:
                    name = inp.get('name', '').lower()
                    if 'dcm' in name or 'doc' in name:
                        value = inp.get('value', '')
                        if value and value.isdigit():
                            dcm_no = value
                            print(f"  âœ… ìˆ¨ê²¨ì§„ input[{inp.get('name')}]ì—ì„œ ë°œê²¬: {dcm_no}")
                            break
            
            # ë°©ë²• 4: ë‹¤ìš´ë¡œë“œ ë²„íŠ¼/ë§í¬ì—ì„œ ì°¾ê¸°
            if not dcm_no:
                download_elements = soup.find_all(['a', 'button'], 
                    string=re.compile(r'ë‹¤ìš´ë¡œë“œ|download|PDF', re.IGNORECASE))
                
                for elem in download_elements:
                    onclick = elem.get('onclick', '')
                    href = elem.get('href', '')
                    
                    # onclickì—ì„œ dcmNo ì¶”ì¶œ (openPdfDownload í•¨ìˆ˜ í˜¸ì¶œì—ì„œ)
                    if onclick:
                        pdf_onclick_match = re.search(r'openPdfDownload\s*\(\s*["\']?(\d+)["\']?\s*,\s*["\']?(\d+)["\']?\s*\)', onclick)
                        if pdf_onclick_match:
                            rcp_param = pdf_onclick_match.group(1)
                            dcm_param = pdf_onclick_match.group(2)
                            dcm_no = dcm_param
                            print(f"  âœ… ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ onclickì—ì„œ ë°œê²¬: dcmNo={dcm_no}")
                            print(f"  ì „ì²´ onclick: {onclick}")
                            break
                    
                    # hrefì—ì„œ dcmNo ì¶”ì¶œ
                    if href and 'dcm' in href.lower():
                        dcm_match = re.search(r'dcm[_-]?no=(\d+)', href, re.IGNORECASE)
                        if dcm_match:
                            dcm_no = dcm_match.group(1)
                            print(f"  âœ… ë‹¤ìš´ë¡œë“œ ë§í¬ hrefì—ì„œ ë°œê²¬: {dcm_no}")
                            break
            
            if rcp_no and dcm_no:
                return {
                    'rcp_no': rcp_no,
                    'dcm_no': dcm_no,
                    'download_page_url': f"https://dart.fss.or.kr/pdf/download/main.do?rcp_no={rcp_no}&dcm_no={dcm_no}",
                    'download_url': f"https://dart.fss.or.kr/pdf/download/pdf.do?rcp_no={rcp_no}&dcm_no={dcm_no}"
                }
            else:
                print(f"  âŒ ë‹¤ìš´ë¡œë“œ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (rcpNo: {rcp_no}, dcmNo: {dcm_no})")
                return None
                
        except Exception as e:
            print(f"  âŒ ë‹¤ìš´ë¡œë“œ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def download_report_file(self, download_info: Dict[str, str], save_dir: str = "downloads") -> bool:
        """ë³´ê³ ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            download_url = download_info['download_url']
            rcp_no = download_info['rcp_no']
            dcm_no = download_info['dcm_no']
            
            print(f"ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„: rcpNo={rcp_no}, dcmNo={dcm_no}")
            
            # ë‹¤ìš´ë¡œë“œ í´ë” ìƒì„±
            os.makedirs(save_dir, exist_ok=True)
            
            # 1ë‹¨ê³„: ë³´ê³ ì„œ í˜ì´ì§€ ë°©ë¬¸í•˜ì—¬ ì„¸ì…˜ ì„¤ì •
            report_page_url = f"https://dart.fss.or.kr/dsaf001/main.do?rcpNo={rcp_no}"
            self.session.get(report_page_url)
            
            # 2ë‹¨ê³„: ë‹¤ìš´ë¡œë“œ í˜ì´ì§€ ë°©ë¬¸ (í•„ìš”ì‹œ)
            if 'download_page_url' in download_info:
                download_page_url = download_info['download_page_url']
                self.session.get(download_page_url)
                referer_url = download_page_url
            else:
                referer_url = report_page_url
            
            # 3ë‹¨ê³„: ì‹¤ì œ PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            headers = {
                'Referer': referer_url,
                'Accept': 'application/pdf,*/*'
            }
            response = self.session.get(download_url, headers=headers, allow_redirects=True, stream=True)
            
            print(f"  ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            print(f"  Content-Type: {response.headers.get('Content-Type', 'ì—†ìŒ')}")
            print(f"  Content-Length: {response.headers.get('Content-Length', 'ì—†ìŒ')}")
            
            if response.status_code == 200:
                content_type = response.headers.get('Content-Type', '')
                
                if 'application/pdf' in content_type:
                    # íŒŒì¼ëª… ìƒì„± - í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
                    content_disposition = response.headers.get('Content-Disposition', '')
                    filename = None
                    
                    if content_disposition:
                        try:
                            # filename*= í˜•íƒœ (RFC 6266) ìš°ì„  ì²˜ë¦¬
                            filename_star_match = re.search(r'filename\*=UTF-8\'\'([^;]+)', content_disposition)
                            if filename_star_match:
                                import urllib.parse
                                filename = urllib.parse.unquote(filename_star_match.group(1))
                                print(f"  ğŸ“ UTF-8 íŒŒì¼ëª…: {filename}")
                            else:
                                # ì¼ë°˜ filename= í˜•íƒœ ì²˜ë¦¬
                                filename_match = re.search(r'filename=["\']?([^"\';\s][^"\';]*)["\']?', content_disposition)
                                if filename_match:
                                    raw_filename = filename_match.group(1)
                                    print(f"  ğŸ” ì›ë³¸ íŒŒì¼ëª…: {repr(raw_filename)}")
                                    
                                    # ì—¬ëŸ¬ ì¸ì½”ë”© ë°©ì‹ ì‹œë„
                                    encodings_to_try = [
                                        ('euc-kr', 'EUC-KR'),      # í•œêµ­ì–´ ì£¼ìš” ì¸ì½”ë”©
                                        ('cp949', 'CP949'),        # í•œêµ­ì–´ í™•ì¥ ì¸ì½”ë”©  
                                        ('utf-8', 'UTF-8'),        # ì¼ë°˜ì ì¸ UTF-8
                                        ('iso-8859-1', 'ISO-8859-1')  # ì„œë²„ ê¸°ë³¸ê°’
                                    ]
                                    
                                    filename = None
                                    for encoding, desc in encodings_to_try:
                                        try:
                                            # ë¨¼ì € ISO-8859-1ë¡œ ë°”ì´íŠ¸í™”í•œ í›„ í•´ë‹¹ ì¸ì½”ë”©ìœ¼ë¡œ ë””ì½”ë”©
                                            decoded = raw_filename.encode('iso-8859-1').decode(encoding)
                                            # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ìœ íš¨ì„± ê²€ì‚¬)
                                            if any('\uac00' <= c <= '\ud7a3' for c in decoded):
                                                filename = decoded
                                                print(f"  âœ… {desc} ë””ì½”ë”© ì„±ê³µ: {filename}")
                                                break
                                        except (UnicodeDecodeError, UnicodeEncodeError):
                                            continue
                                    
                                    # ëª¨ë“  ë””ì½”ë”©ì´ ì‹¤íŒ¨í•œ ê²½ìš° ì›ë³¸ ì‚¬ìš©
                                    if not filename:
                                        filename = raw_filename
                                        print(f"  âš ï¸  ì›ë³¸ íŒŒì¼ëª… ì‚¬ìš©: {filename}")
                        except Exception as e:
                            print(f"  âŒ íŒŒì¼ëª… ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    
                    # ê¸°ë³¸ íŒŒì¼ëª… ì„¤ì •
                    if not filename:
                        filename = f"report_{rcp_no}_{dcm_no}.pdf"
                    
                    # íŒŒì¼ëª… ì •ë¦¬ (ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±)
                    import string
                    # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ê¸°í˜¸ë§Œ í—ˆìš©
                    safe_filename = ""
                    for c in filename:
                        if (c.isalnum() or c in "-_.() []" or 
                            '\uac00' <= c <= '\ud7a3' or  # í•œê¸€ ìŒì ˆ
                            '\u3131' <= c <= '\u318e'):   # í•œê¸€ ìëª¨
                            safe_filename += c
                    
                    filename = safe_filename if safe_filename else f"report_{rcp_no}_{dcm_no}.pdf"
                    
                    # PDF í™•ì¥ì í™•ì¸
                    if not filename.lower().endswith('.pdf'):
                        filename += '.pdf'
                        
                    print(f"  ğŸ“ ìµœì¢… íŒŒì¼ëª…: {filename}")
                    
                    # íŒŒì¼ ì €ì¥
                    file_path = os.path.join(save_dir, filename)
                    
                    with open(file_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                    
                    print(f"  âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_path}")
                    return True
                    
                else:
                    print(f"  âš ï¸ PDFê°€ ì•„ë‹Œ ì‘ë‹µ: {content_type}")
                    # HTML ì‘ë‹µì¸ ê²½ìš° ë¡œê·¸ì¸ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±
                    if 'text/html' in content_type:
                        print(f"  ğŸ“„ HTML ì‘ë‹µ - ë¡œê·¸ì¸ì´ í•„ìš”í•˜ê±°ë‚˜ ë‹¤ë¥¸ ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ")
                    return False
            else:
                print(f"  âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            print(f"  âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def download_all_reports_from_txt(self, txt_file: str, save_dir: str = "downloads") -> None:
        """TXT íŒŒì¼ì˜ ëª¨ë“  ë§í¬ì—ì„œ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ"""
        try:
            print(f"ğŸ“ ë§í¬ íŒŒì¼ ì½ê¸°: {txt_file}")
            
            if not os.path.exists(txt_file):
                print(f"âŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {txt_file}")
                return
            
            # ë§í¬ ì¶”ì¶œ
            links = []
            with open(txt_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith('https://dart.fss.or.kr/dsaf001/main.do?rcpNo='):
                        links.append(line)
            
            if not links:
                print("âŒ ìœ íš¨í•œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return
            
            print(f"ğŸ” ì´ {len(links)}ê°œ ë§í¬ ë°œê²¬")
            print(f"ğŸ“ ë‹¤ìš´ë¡œë“œ í´ë”: {save_dir}")
            print("=" * 80)
            
            # ë‹¤ìš´ë¡œë“œ í†µê³„
            success_count = 0
            fail_count = 0
            
            for i, link in enumerate(links, 1):
                print(f"\n[{i:2d}/{len(links)}] ì²˜ë¦¬ ì¤‘...")
                
                # ë‹¤ìš´ë¡œë“œ ì •ë³´ ì¶”ì¶œ
                download_info = self.get_report_download_info(link)
                
                if download_info:
                    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                    if self.download_report_file(download_info, save_dir):
                        success_count += 1
                    else:
                        fail_count += 1
                else:
                    fail_count += 1
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
                if i < len(links):
                    time.sleep(1)
            
            # ê²°ê³¼ ìš”ì•½
            print("\n" + "=" * 80)
            print(f"ğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            print(f"  âœ… ì„±ê³µ: {success_count}ê±´")
            print(f"  âŒ ì‹¤íŒ¨: {fail_count}ê±´")
            print(f"  ğŸ“ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(save_dir)}")
            
        except Exception as e:
            print(f"âŒ ì¼ê´„ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def search_by_criteria(self, company: str, start_date: str, end_date: str, report_name: str = "") -> List[Dict]:
        """ì¡°ê±´ì— ë”°ë¥¸ ë³´ê³ ì„œ ê²€ìƒ‰"""
        try:
            print(f"ğŸ” DART ê²€ìƒ‰: {company} - {report_name}")
            
            # ê²€ìƒ‰ ìš”ì²­ ë°ì´í„° êµ¬ì„±
            search_data = {
                'currentPage': '1',
                'maxResults': '100',
                'maxLinks': '10',
                'sort': 'rcpDt',
                'series': 'desc',
                'textCrpNm': company,
                'startDate': start_date,
                'endDate': end_date,
            }
            
            # ë³´ê³ ì„œ ìœ í˜•ì´ ì§€ì •ëœ ê²½ìš° ì¶”ê°€
            if report_name:
                search_data['textRptNm'] = report_name
            
            # DART ê²€ìƒ‰ ìš”ì²­
            response = self.session.post(self.search_url, data=search_data)
            
            if response.status_code == 200:
                # HTML íŒŒì‹±í•˜ì—¬ ê²°ê³¼ ì¶”ì¶œ
                results = self._parse_search_results(response.text)
                
                # ë³´ê³ ì„œ ìœ í˜•ë³„ í•„í„°ë§
                if report_name:
                    filtered_results = []
                    for result in results:
                        if report_name in result.get('report_name', ''):
                            filtered_results.append(result)
                    return filtered_results
                else:
                    return results
            else:
                print(f"  âŒ ê²€ìƒ‰ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"  âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def search_company(self, company_name: str) -> List[Dict[str, str]]:
        """íšŒì‚¬ëª…ìœ¼ë¡œ íšŒì‚¬ ê²€ìƒ‰"""
        try:
            print(f"ğŸ” íšŒì‚¬ ê²€ìƒ‰: {company_name}")
            
            # ê°„ë‹¨í•œ íšŒì‚¬ ê²€ìƒ‰ êµ¬í˜„ - ì…ë ¥ë°›ì€ íšŒì‚¬ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            # ì‹¤ì œë¡œëŠ” DART APIë¥¼ í†µí•´ ì •í™•í•œ íšŒì‚¬ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì•¼ í•¨
            companies = [{
                'name': company_name,
                'stock_code': 'N/A',
                'corp_code': 'N/A'
            }]
            
            print(f"   âœ… íšŒì‚¬ ì°¾ìŒ: {company_name}")
            return companies
                
        except Exception as e:
            print(f"âŒ íšŒì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def search_reports(self, company_name: str, start_date: str, end_date: str, report_types: List[str]) -> List[Dict[str, str]]:
        """ì •ê¸°ê³µì‹œ ë³´ê³ ì„œ ê²€ìƒ‰"""
        try:
            print(f"ğŸ“Š ì •ê¸°ê³µì‹œ ê²€ìƒ‰: {company_name}")
            print(f"   ê¸°ê°„: {start_date} ~ {end_date}")
            print(f"   ìœ í˜•: {', '.join(report_types)}")
            
            # ê¸°ì¡´ì˜ search_company_regular_reports ë©”ì„œë“œ í™œìš©
            print("   ğŸ” ì •ê¸°ê³µì‹œ ë³´ê³ ì„œ ê²€ìƒ‰ ì¤‘...")
            
            # ê¸°ê°„ ê³„ì‚° (ë…„ìˆ˜ë¡œ ë³€í™˜)
            from datetime import datetime
            start = datetime.strptime(start_date, '%Y%m%d')
            end = datetime.strptime(end_date, '%Y%m%d')
            years = max(1, (end - start).days // 365)
            
            reports = self.search_company_regular_reports(company_name, max_pages=10)
            
            if reports:
                print(f"     âœ… {len(reports)}ê±´ ë°œê²¬")
                print(f"ğŸ“Š ì´ {len(reports)}ê°œ ë³´ê³ ì„œ ë°œê²¬")
                return reports
            else:
                print(f"     â„¹ï¸  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                print(f"ğŸ“Š ì´ 0ê°œ ë³´ê³ ì„œ ë°œê²¬")
                return []
            
        except Exception as e:
            print(f"âŒ ë³´ê³ ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def download_reports_batch(self, reports: List[Dict[str, str]], download_dir: str):
        """ë³´ê³ ì„œ ëª©ë¡ ì¼ê´„ ë‹¤ìš´ë¡œë“œ"""
        try:
            if not reports:
                print("âŒ ë‹¤ìš´ë¡œë“œí•  ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            print(f"\nğŸ“¥ {len(reports)}ê°œ ë³´ê³ ì„œ ì¼ê´„ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
            print(f"ğŸ“ ë‹¤ìš´ë¡œë“œ í´ë”: {download_dir}")
            print("=" * 80)
            
            success_count = 0
            fail_count = 0
            
            for i, report in enumerate(reports, 1):
                print(f"\n[{i:2d}/{len(reports)}] ì²˜ë¦¬ ì¤‘...")
                
                # ë³´ê³ ì„œ URL ì¶”ì¶œ
                report_url = report.get('report_url')
                if not report_url:
                    print("  âŒ ë³´ê³ ì„œ URLì´ ì—†ìŠµë‹ˆë‹¤.")
                    fail_count += 1
                    continue
                
                # ë‹¤ìš´ë¡œë“œ ì •ë³´ ì¶”ì¶œ
                download_info = self.get_report_download_info(report_url)
                
                if download_info:
                    # PDF ë‹¤ìš´ë¡œë“œ
                    if self.download_report_file(download_info, download_dir):
                        success_count += 1
                    else:
                        fail_count += 1
                else:
                    fail_count += 1
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
                time.sleep(1)
            
            print("\n" + "=" * 80)
            print("ğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            print(f"  âœ… ì„±ê³µ: {success_count}ê±´")
            print(f"  âŒ ì‹¤íŒ¨: {fail_count}ê±´")
            if download_dir:
                print(f"  ğŸ“ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(download_dir)}")
                
        except Exception as e:
            print(f"âŒ ì¼ê´„ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ DART ê³µì‹œì •ë³´ ê²€ìƒ‰ í”„ë¡œê·¸ë¨")
    print("=" * 50)
    
    scraper = DartScraper()
    
    # ì´ˆê¸°í™”
    if not scraper.get_search_page():
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    try:
        while True:
            print("\n" + "=" * 50)
            company_name = input("ğŸ¢ ê²€ìƒ‰í•  íšŒì‚¬ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: q): ").strip()
            
            if company_name.lower() == 'q':
                print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not company_name:
                print("âš ï¸ íšŒì‚¬ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ë³´ê³ ì„œ í•„í„° ì˜µì…˜ ì„ íƒ
            filter_choice = input("ğŸ“‹ íŠ¹ì • ë³´ê³ ì„œë§Œ ê²€ìƒ‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ë°˜ê¸°ë³´ê³ ì„œ, ë¶„ê¸°ë³´ê³ ì„œ, ì‚¬ì—…ë³´ê³ ì„œ) (y/n, ê¸°ë³¸ê°’: n): ").strip().lower()
            filter_reports = filter_choice == 'y'
            
            # ê²€ìƒ‰ ì‹¤í–‰ (ìµœëŒ€ 5í˜ì´ì§€ê¹Œì§€)
            results = scraper.search_company(company_name, years=10, filter_reports=filter_reports, max_pages=5)
            
            # ê²°ê³¼ ì¶œë ¥
            scraper.display_results(results)
            
            # ê²°ê³¼ ì €ì¥ ì—¬ë¶€ í™•ì¸
            if results:
                print("\nğŸ’¾ ì €ì¥ ì˜µì…˜:")
                print("   1) JSON íŒŒì¼ë¡œ ì €ì¥ (ì „ì²´ ì •ë³´)")
                print("   2) ë§í¬ë§Œ TXT íŒŒì¼ë¡œ ì €ì¥")
                print("   3) ë‘˜ ë‹¤ ì €ì¥")
                print("   4) ì €ì¥í•˜ì§€ ì•ŠìŒ")
                
                save_choice = input("ì„ íƒí•˜ì„¸ìš” (1-4, ê¸°ë³¸ê°’: 4): ").strip()
                
                if save_choice == '1':
                    scraper.save_results(results)
                elif save_choice == '2':
                    scraper.save_links_to_txt(results)
                elif save_choice == '3':
                    scraper.save_results(results)
                    scraper.save_links_to_txt(results)
                else:
                    print("ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            # ì ì‹œ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâœ— ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()